{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python Debugger: Current File",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal"
        },
        {
            "name": "Train model - Local debugging - with wandb",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}",
            "args": [
                // "dataset=mod_subtract_dataset",
                "dataset=mod_sum_dataset",
                "dataset.p=97",
                "dataset.frac_train=0.4",
                "dataset.dataset_seed=42",
                //
                "train.load_checkpoint_from_dir=null", // <-- Train from scratch if no checkpoint dir is given (null corresponds to None in hydra)
                // "train.load_checkpoint_from_dir='outputs/run/train_grokk/2025-04-24/14-48-17.709719/checkpoints/step=799'", // <-- Load from checkpoint dir
                //
                // "train.weight_decay=0.0", // <-- No weight decay
                "train.weight_decay=0.01", // <-- Regular weight decay
                // "train.weight_decay=0.1", // <-- Large weight decay
                //
                "train.save_checkpoints_every=200",
                "train.max_steps=250000", // <-- 250k steps
                "topological_analysis.compute_estimates_every=200", // <-- compute local estimates only in large intervals
                "topological_analysis.create_projection_plot_every=5000", // <-- For debugging, create projection plots more regularly
                //
                "wandb.use_wandb=true",
                // "wandb.use_wandb=false",
                "wandb.wandb_project=grokking_replica_debugging",
                "wandb.wandb_notes='Debugging the training process'",
            ],
        },
        {
            "name": "Train model - Local debugging - offline mode with wandb",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}",
            "args": [
                "dataset.dataset_seed=42",
                //
                "dataset.frac_train=0.4",
                //
                "wandb.use_wandb=true",
                "wandb.wandb_project=grokking_replica_debugging",
            ],
            "env": {
                "WANDB_MODE": "offline"
            }
        },
        {
            "name": "Train model - Local debugging - default settings - mod_subtract_dataset - without wandb",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}",
            "args": [
                "dataset.dataset_seed=42",
                //
                "dataset.frac_train=0.4",
                //
                "train.weight_decay=0.0",
                "train.save_checkpoints_every=50", // <-- For debugging the saving function: Save checkpoints more regularly 
                //
                "wandb.use_wandb=false",
                "wandb.wandb_project=grokking_replica_debugging", // <-- not relevant for running without wandb
            ],
            "env": {
                "WANDB_DISABLED": "true"
            }
        },
        {
            "name": "Train model - Local debugging - default settings - mod_sum_dataset - without wandb",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}",
            "args": [
                "dataset=mod_sum_dataset",
                "dataset.dataset_seed=42",
                "dataset.frac_train=0.4",
                //
                "train.weight_decay=0.0",
                "train.save_checkpoints_every=50", // <-- For debugging the saving function: Save checkpoints more regularly 
                //
                "logging.training.log_example_batch_every=50", // <-- For debugging the logging function: Log example batches more regularly
                //
                "wandb.use_wandb=false",
                "wandb.wandb_project=grokking_replica_debugging", // <-- not relevant for running without wandb
            ],
            "env": {
                "WANDB_DISABLED": "true"
            }
        },
        {
            "name": "Train model - Local debugging - load from checkpoint dir - without wandb",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}",
            "args": [
                // "train.load_checkpoint_from_dir=null", // <-- Train from scratch if no checkpoint dir is given (null corresponds to None in hydra)
                "train.load_checkpoint_from_dir='outputs/run/train_grokk/2025-04-23/11-52-38/checkpoints/step=300'", // <-- Load from checkpoint dir
                //
                "train.save_checkpoints_every=50", // <-- For debugging the saving function: Save checkpoints more regularly 
                //
                "logging.training.log_example_batch_every=50", // <-- For debugging the logging function: Log example batches more regularly
                //
                "wandb.use_wandb=false",
                "wandb.wandb_project=grokking_replica_debugging", // <-- not relevant for running without wandb
            ],
            "env": {
                "WANDB_DISABLED": "true"
            }
        },
        {
            "name": "Train model - Local debugging - multirun - without wandb",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}",
            "args": [
                "--multirun",
                "hydra/launcher=basic",
                // "hydra/launcher=joblib", // <-- Note: Currently, the joblib launcher is not working locally.
                //
                "dataset.frac_train=0.4",
                //
                "train.weight_decay=0.01",
                //
                "wandb.use_wandb=false",
                "wandb.wandb_project=grokking_replica_debugging", // <-- not relevant for running without wandb
            ],
            "env": {
                "WANDB_DISABLED": "true"
            }
        },
        // Note: Even though this page claims that the maximum duration of a GPU-Job is 47:59:59, we can sucessfully submit jobs with a duration of 59:00:00.
        // https://wiki.hhu.de/display/HPC/Besonderheiten
        {
            "name": "Train model - HHU Hilbert HPC submission - Single Setup - without wandb",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "args": [
                //    >> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=hpc_submission", // <-- Use this for HHU Hilbert HPC submission
                // >> DSML nodes:
                "hydra.launcher.queue=DSML",
                "hydra.launcher.template=DSML",
                // >> GTX1080 nodes:
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=GTX1080",
                // >> RTX6000 nodes:
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=2",
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=59:00:00", // <-- We choose to run the training for long enough so that we can get up to ... global steps.
                //    >> END: Hydra options
                //
                "dataset.dataset_seed=42",
                //
                "dataset.frac_train=0.4",
                //
                "train.weight_decay=0.01",
                //
                "wandb.use_wandb=false",
                "wandb.wandb_project=without_wandb",
            ],
            "env": {
                "WANDB_DISABLED": "true",
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Train model - HHU Hilbert HPC submission - Single Setup - with wandb",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "args": [
                //    >> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                // "hydra/launcher=basic", // <-- Use this for local submission
                "hydra/launcher=hpc_submission", // <-- Use this for HHU Hilbert HPC submission
                // >> DSML nodes:
                // "hydra.launcher.queue=DSML",
                // "hydra.launcher.template=DSML",
                // >> GTX1080 nodes:
                "hydra.launcher.queue=CUDA",
                "hydra.launcher.template=GTX1080",
                // >> RTX6000 nodes:
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=4",
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=59:00:00", // <-- We choose to run the training for long enough so that we can get up to ... global steps.
                //    >> END: Hydra options
                //
                "dataset.dataset_seed=42",
                //
                "dataset.frac_train=0.4",
                //
                "wandb.use_wandb=true",
                "wandb.wandb_project=grokking_replica_HHU_Hilbert_HPC_runs",
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Train model - HHU Hilbert HPC submission - Multiple dataset seeds",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "args": [
                //    >> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                // "hydra/launcher=basic", // <-- Use this for local submission
                "hydra/launcher=hpc_submission", // <-- Use this for HHU Hilbert HPC submission
                // >> DSML nodes:
                // "hydra.launcher.queue=DSML",
                // "hydra.launcher.template=DSML",
                // >> GTX1080 nodes:
                "hydra.launcher.queue=CUDA",
                "hydra.launcher.template=GTX1080",
                // >> RTX6000 nodes:
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=4",
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=59:00:00", // <-- We choose to run the training for long enough so that we can get up to ... global steps.
                //    >> END: Hydra options
                //
                // "dataset.dataset_seed=42",
                // "dataset.dataset_seed=43,44", // <-- Two additional seeds
                "dataset.dataset_seed=43,44,45,46", // <-- Four additional seeds
                // "dataset.dataset_seed=42,43,44,45,46", // <-- Five seeds
                //
                "dataset.frac_train=0.4",
                //
                "train.weight_decay=0.01",
                //
                "wandb.use_wandb=true",
                "wandb.wandb_project=grokking_replica_HHU_Hilbert_HPC_runs",
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Train model - HHU Hilbert HPC submission - Very long runs; Multiple dataset seeds; With and without weight decay",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "args": [
                //    >> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                // "hydra/launcher=basic", // <-- Use this for local submission
                "hydra/launcher=hpc_submission", // <-- Use this for HHU Hilbert HPC submission
                // >> DSML nodes:
                "hydra.launcher.queue=DSML",
                "hydra.launcher.template=DSML",
                // >> GTX1080 nodes:
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=GTX1080",
                // >> RTX6000 nodes:
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=2",
                "hydra.launcher.ngpus=1",
                // Notes:
                // - Walltime of 72:00:00 works on our DSML nodes.
                // - In general, on HHU Hilbert HPC, there is a walltime limit for GPU jobs.
                "hydra.launcher.walltime=72:00:00", // <-- We choose to run the training for long enough so that we can get up to 500k global steps.
                //    >> END: Hydra options
                //
                // "dataset.dataset_seed=42",
                // "dataset.dataset_seed=43,44", // <-- Two additional seeds
                // "dataset.dataset_seed=43,44,45,46", // <-- Four additional seeds
                // "dataset.dataset_seed=42,43,44,45,46", // <-- Five seeds
                "dataset.dataset_seed=47,48",
                //
                "dataset.frac_train=0.4",
                //
                "train.weight_decay=0.0,0.01", // <-- Two different weight decay values
                //
                // > Parameters for very long runs
                "train.save_checkpoints_every=50000",
                "train.max_steps=5000000", // <-- 5 million steps
                "topological_analysis.compute_estimates_every=500", // <-- compute local estimates only in large intervals
                "topological_analysis.create_projection_plot_every=10000", // <-- create projection plot only in large intervals
                //
                "wandb.use_wandb=true",
                "wandb.wandb_project=grokking_replica_HHU_Hilbert_HPC_runs_very_long",
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Train model - HHU Hilbert HPC submission - Different operations and p values; Long runs; Multiple dataset seeds",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "args": [
                //    >> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                // "hydra/launcher=basic", // <-- Use this for local submission
                "hydra/launcher=hpc_submission", // <-- Use this for HHU Hilbert HPC submission
                // >> DSML nodes:
                "hydra.launcher.queue=DSML",
                "hydra.launcher.template=DSML",
                // >> GTX1080 nodes:
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=GTX1080",
                // >> RTX6000 nodes:
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=2",
                "hydra.launcher.ngpus=1",
                // Notes:
                // - Walltime of 72:00:00 works on our DSML nodes.
                // - In general, on HHU Hilbert HPC, there is a walltime limit for GPU jobs.
                "hydra.launcher.walltime=72:00:00", // <-- We choose to run the training for long enough so that we can get up to 500k global steps.
                //    >> END: Hydra options
                //
                "dataset=mod_subtract_dataset,mod_sum_dataset",
                "dataset.p=96,97",
                //
                // "dataset.dataset_seed=42",
                "dataset.dataset_seed=43,44,45", // <-- Three different seeds
                //
                "dataset.frac_train=0.4",
                //
                "train.weight_decay=0.01", // <-- Small weight decay
                //
                // > Parameters for long runs
                "train.max_steps=400000", // <-- 400k steps (everything interesting should happen before that)
                "train.save_checkpoints_every=10000",
                "topological_analysis.compute_estimates_every=500", // <-- compute local estimates only in large intervals
                "topological_analysis.create_projection_plot_every=10000", // <-- create projection plot only in large intervals
                //
                "wandb.use_wandb=true",
                "wandb.wandb_project=grokking_replica_HHU_Hilbert_HPC_runs_different_operations_and_p_values_long",
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
    ]
}