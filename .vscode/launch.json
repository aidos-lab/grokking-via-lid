{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python Debugger: Current File",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal"
        },
        {
            "name": "Train model - Local debugging - with wandb",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}",
            "args": [
                // "dataset=mod_multiply_dataset",
                "dataset=mod_subtract_dataset",
                // "dataset=mod_sum_dataset",
                "dataset.p=97",
                "dataset.frac_train=0.4",
                "dataset.dataset_seed=42",
                //
                "train.load_checkpoint_from_dir=null", // <-- Train from scratch if no checkpoint dir is given (null corresponds to None in hydra)
                // "train.load_checkpoint_from_dir='outputs/run/train_grokk/2025-04-24/14-48-17.709719/checkpoints/step=799'", // <-- Example: Load from checkpoint dir
                //
                // "train.optimizer.weight_decay=0.0", // <-- No weight decay
                "train.optimizer.weight_decay=0.01", // <-- Regular weight decay
                // "train.optimizer.weight_decay=0.1", // <-- Large weight decay
                "train.optimizer.eps=1e-6",
                // "train.optimizer.clip_grad_norm_max_norm=null",
                "train.optimizer.clip_grad_norm_max_norm=1.0",
                //
                "train.lr_scheduler.lr_scheduler_type=constant",
                // "train.lr_scheduler.lr_scheduler_type=linear",
                //
                "train.save_checkpoints_every=10000",
                "train.max_steps=20000", // <-- 20k steps for debugging learning rate scheduler
                // "train.max_steps=250000", // <-- 250k steps
                //
                // "topological_analysis.compute_estimates_every=200",
                "topological_analysis.compute_estimates_every=1000", // <-- compute local estimates only in large intervals
                //
                // "topological_analysis.absolute_n_neighbors_choices=[64]",
                // "topological_analysis.absolute_n_neighbors_choices=[32,64,128]",
                //
                // "topological_analysis.create_projection_plot_every=500", // <-- For debugging, create projection plots more regularly
                "topological_analysis.create_projection_plot_every=2000",
                // "topological_analysis.create_projection_plot_every=10000",
                // "logging.training.create_plot_of_model_output_parameters_every=500", // <-- For debugging the logging function: Create plots more regularly
                "logging.training.create_plot_of_model_output_parameters_every=2000",
                // "logging.training.create_plot_of_model_output_parameters_every=10000",
                //
                "wandb.use_wandb=true",
                // "wandb.watch.log=gradients",
                // "wandb.watch.log=parameters",
                "wandb.watch.log=all",
                "wandb.wandb_project=grokking_replica_debugging",
                "wandb.wandb_notes='Debugging the training process'",
            ],
        },
        {
            "name": "Train model - Local debugging - offline mode with wandb",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}",
            "args": [
                "dataset.dataset_seed=42",
                //
                "dataset.frac_train=0.4",
                //
                "wandb.use_wandb=true",
                "wandb.wandb_project=grokking_replica_debugging",
            ],
            "env": {
                "WANDB_MODE": "offline"
            }
        },
        {
            "name": "Train model - Local debugging - default settings - mod_subtract_dataset - without wandb",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}",
            "args": [
                "dataset.dataset_seed=42",
                "dataset.p=97",
                //
                "dataset.frac_train=0.4",
                //
                "train.load_checkpoint_from_dir=null", // <-- Train from scratch if no checkpoint dir is given (null corresponds to None in hydra)
                // "train.load_checkpoint_from_dir='outputs/run/train_grokk/2025-05-03/12-09-39.284931/checkpoints/step=2499'", // <-- Example: Load from checkpoint dir
                //
                "train.optimizer.weight_decay=0.01",
                // "train.optimizer.eps=1e-8", // eps=1e-8 is the default value in AdamW optimizer
                "train.optimizer.eps=1e-6",
                //
                // "train.lr_scheduler.lr_scheduler_type=constant",
                "train.lr_scheduler.lr_scheduler_type=linear",
                //
                // "train.save_checkpoints_every=500", // <-- For debugging the saving function: Save checkpoints more regularly 
                "train.save_checkpoints_every=5000", // <-- For debugging the saving function: Save checkpoints more regularly 
                //
                // "topological_analysis.compute_estimates_every=200",
                "topological_analysis.compute_estimates_every=500",
                //
                "logging.training.log_example_batch_every=200", // <-- For debugging the logging function: Log example batches more regularly
                "logging.training.create_plot_of_model_output_parameters_every=500", // <-- For debugging the logging function: Create plots more regularly
                //
                "wandb.use_wandb=false",
                "wandb.wandb_project=grokking_replica_debugging", // <-- not relevant for running without wandb
            ],
            "env": {
                "WANDB_DISABLED": "true"
            }
        },
        {
            "name": "Train model - Local debugging - default settings - choose a different dataset - without wandb",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}",
            "args": [
                "dataset=mod_sum_dataset",
                // "dataset=mod_multiply_dataset",
                // "dataset=mod_division_dataset",
                "dataset.p=97", // <-- For division, it makes sense to choose this as a prime number.
                //
                "dataset.dataset_seed=42",
                "dataset.frac_train=0.4",
                //
                "train.optimizer.weight_decay=0.01",
                "train.save_checkpoints_every=10000", // <-- For debugging the saving function: Save checkpoints more regularly 
                //
                // "topological_analysis.compute_estimates_every=200",
                "topological_analysis.compute_estimates_every=1000",
                //
                "logging.training.log_example_batch_every=200", // <-- For debugging the logging function: Log example batches more regularly
                // "logging.training.create_plot_of_model_output_parameters_every=200", // <-- For debugging the logging function: Create plots more regularly
                "logging.training.create_plot_of_model_output_parameters_every=1000", // <-- For debugging the logging function: Create plots more regularly
                //
                "wandb.use_wandb=false",
                "wandb.wandb_project=grokking_replica_debugging", // <-- not relevant for running without wandb
            ],
            "env": {
                "WANDB_DISABLED": "true"
            }
        },
        {
            "name": "Train model - Local debugging - load from checkpoint dir - without wandb",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}",
            "args": [
                // "train.load_checkpoint_from_dir=null", // <-- Train from scratch if no checkpoint dir is given (null corresponds to None in hydra)
                "train.load_checkpoint_from_dir='outputs/run/train_grokk/2025-04-23/11-52-38/checkpoints/step=300'", // <-- Example: Load from checkpoint dir
                //
                "train.save_checkpoints_every=50", // <-- For debugging the saving function: Save checkpoints more regularly 
                //
                "logging.training.log_example_batch_every=50", // <-- For debugging the logging function: Log example batches more regularly
                //
                "wandb.use_wandb=false",
                "wandb.wandb_project=grokking_replica_debugging", // <-- not relevant for running without wandb
            ],
            "env": {
                "WANDB_DISABLED": "true"
            }
        },
        {
            "name": "Train model - Local debugging - multirun - without wandb",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "cwd": "${workspaceFolder}",
            "args": [
                "--multirun",
                "hydra/launcher=basic",
                // "hydra/launcher=joblib", // <-- Note: Currently, the joblib launcher is not working locally.
                //
                "dataset.frac_train=0.4",
                //
                "train.optimizer.weight_decay=0.01",
                //
                "wandb.use_wandb=false",
                "wandb.wandb_project=grokking_replica_debugging", // <-- not relevant for running without wandb
            ],
            "env": {
                "WANDB_DISABLED": "true"
            }
        },
        {
            "name": "Train model - HPC cluster submission - Single Setup - without wandb",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "args": [
                //    >> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                "hydra/launcher=hpc_submission", // <-- Use this for HPC cluster submission
                // >> GTX1080 nodes:
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=GTX1080",
                // >> RTX6000 nodes:
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=2", // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=59:59:59", // <-- We choose to run the training for long enough so that we can get up to 400k global steps.
                //    >> END: Hydra options
                //
                "dataset.dataset_seed=42",
                //
                "dataset.frac_train=0.4",
                //
                "train.optimizer.weight_decay=0.01",
                //
                "wandb.use_wandb=false",
                "wandb.wandb_project=without_wandb",
            ],
            "env": {
                "WANDB_DISABLED": "true",
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Train model - HPC cluster submission - Single Setup - with wandb",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "args": [
                //    >> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                // "hydra/launcher=basic", // <-- Use this for local submission
                "hydra/launcher=hpc_submission", // <-- Use this for HPC cluster submission
                // >> GTX1080 nodes:
                "hydra.launcher.queue=CUDA",
                "hydra.launcher.template=GTX1080",
                // >> RTX6000 nodes:
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=2", // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=59:59:59", // <-- We choose to run the training for long enough so that we can get up to 400k global steps.
                //    >> END: Hydra options
                //
                "dataset.dataset_seed=42",
                //
                "dataset.frac_train=0.4",
                //
                "wandb.use_wandb=true",
                "wandb.wandb_project=grokking_replica_HPC_cluster_runs",
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Train model - HPC cluster submission - Multiple dataset seeds",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "args": [
                //    >> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                // "hydra/launcher=basic", // <-- Use this for local submission
                "hydra/launcher=hpc_submission", // <-- Use this for HPC cluster submission
                // >> GTX1080 nodes:
                "hydra.launcher.queue=CUDA",
                "hydra.launcher.template=GTX1080",
                // >> RTX6000 nodes:
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=2", // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                "hydra.launcher.ngpus=1",
                "hydra.launcher.walltime=59:59:59", // <-- We choose to run the training for long enough so that we can get up to 400k global steps.
                //    >> END: Hydra options
                //
                // "dataset.dataset_seed=42",
                // "dataset.dataset_seed=43,44", // <-- Two additional seeds
                "dataset.dataset_seed=43,44,45,46", // <-- Four additional seeds
                // "dataset.dataset_seed=42,43,44,45,46", // <-- Five seeds
                //
                "dataset.frac_train=0.4",
                //
                "train.optimizer.weight_decay=0.01",
                //
                "wandb.use_wandb=true",
                "wandb.wandb_project=grokking_replica_HPC_cluster_runs",
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Train model - HPC cluster submission - Very long runs; Multiple dataset seeds; With and without weight decay",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "args": [
                //    >> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                // "hydra/launcher=basic", // <-- Use this for local submission
                "hydra/launcher=hpc_submission", // <-- Use this for HPC cluster submission
                // >> GTX1080 nodes:
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=GTX1080",
                // >> RTX6000 nodes:
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=2", // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                "hydra.launcher.ngpus=1",
                // Notes:
                // - Walltime of 72:00:00 works on our nodes.
                // - In general, on HPC cluster, there is a walltime limit for GPU jobs.
                "hydra.launcher.walltime=72:00:00", // <-- We choose to run the training for long enough so that we can get up to 500k global steps.
                //    >> END: Hydra options
                //
                // "dataset.dataset_seed=42",
                // "dataset.dataset_seed=43,44", // <-- Two additional seeds
                // "dataset.dataset_seed=43,44,45,46", // <-- Four additional seeds
                // "dataset.dataset_seed=42,43,44,45,46", // <-- Five seeds
                "dataset.dataset_seed=47,48",
                //
                "dataset.frac_train=0.4",
                //
                "train.optimizer.weight_decay=0.0,0.01", // <-- Two different weight decay values
                //
                // > Parameters for very long runs
                "train.save_checkpoints_every=50000",
                "train.max_steps=5000000", // <-- 5 million steps
                "topological_analysis.compute_estimates_every=500", // <-- compute local estimates only in large intervals
                "topological_analysis.create_projection_plot_every=10000", // <-- create projection plot only in large intervals
                //
                "wandb.use_wandb=true",
                "wandb.wandb_project=grokking_replica_HPC_cluster_runs_very_long",
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Train model - HPC cluster submission - Different operations and p values; Long runs; Multiple dataset seeds",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "args": [
                //    >> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                // "hydra/launcher=basic", // <-- Use this for local submission
                "hydra/launcher=hpc_submission", // <-- Use this for HPC cluster submission
                // >> GTX1080 nodes:
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=GTX1080",
                // >> RTX6000 nodes:
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=2", // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                "hydra.launcher.ngpus=1",
                // Notes:
                // - Walltime of 72:00:00 works on our nodes.
                // - In general, on HPC cluster, there is a walltime limit for GPU jobs.
                "hydra.launcher.walltime=72:00:00", // <-- We choose to run the training for long enough so that we can get up to 500k global steps.
                //    >> END: Hydra options
                //
                // "dataset=mod_sum_dataset",
                // "dataset=mod_multiply_dataset",
                "dataset=mod_subtract_dataset",
                // "dataset=mod_subtract_dataset,mod_sum_dataset",
                // "dataset=mod_subtract_dataset,mod_sum_dataset,mod_division_dataset", // <-- Make sure p is prime (e.g., p=97 works) for division.
                "dataset.p=97",
                //
                // "dataset.dataset_seed=42",
                // "dataset.dataset_seed=43,44,45", // <-- Three different seeds
                // "dataset.dataset_seed=46,47",
                "dataset.dataset_seed=48",
                // "dataset.dataset_seed=46,47,48",
                // "dataset.dataset_seed=49",
                //
                "dataset.frac_train=0.4",
                //
                "train.optimizer.weight_decay=0.01", // <-- Small weight decay
                // "train.optimizer.eps=1e-8", // <-- eps=1e-8 is the default value in AdamW optimizer
                "train.optimizer.eps=1e-6", // <-- We set eps to a larger value to see if this helps with instabilities in the training process
                // "train.optimizer.eps=1e-8,1e-7,1e-6", // <-- Trying out different eps values
                // "train.optimizer.clip_grad_norm_max_norm=1.0",
                "train.optimizer.clip_grad_norm_max_norm=null,1.0", // <-- Trying out different values for clip_grad_norm_max_norm
                //
                // > Parameters for long runs
                // "train.max_steps=800000", // <-- 800k steps (everything interesting should happen before that)
                "train.max_steps=1500000", // <-- 1.5 million steps
                // "train.max_steps=2000000", // <-- 2 million steps do not finish in time for walltime 72:00:00 and topological_analysis.compute_estimates_every=500
                "train.save_checkpoints_every=50000",
                //
                // "logging.training.create_plot_of_model_output_parameters_every=5000",
                "logging.training.create_plot_of_model_output_parameters_every=10000",
                //
                "topological_analysis.compute_estimates_every=500", // <-- compute local estimates only in large intervals
                "topological_analysis.create_projection_plot_every=20000", // <-- create projection plot only in large intervals
                //
                "wandb.use_wandb=true",
                "wandb.wandb_project=grokking_replica_HPC_cluster_runs_different_operations_and_p_values_long_with_regularization",
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
        {
            "name": "Train model - HPC cluster submission - Different dataset portions; Long runs; Multiple dataset seeds",
            "type": "debugpy",
            "request": "launch",
            "program": "grokking/scripts/train_grokk.py",
            "console": "integratedTerminal",
            "args": [
                //    >> START: Hydra options
                "--multirun",
                "hydra/sweeper=basic",
                // "hydra/launcher=basic", // <-- Use this for local submission
                "hydra/launcher=hpc_submission", // <-- Use this for HPC cluster submission
                // >> GTX1080 nodes:
                "hydra.launcher.queue=CUDA",
                "hydra.launcher.template=GTX1080",
                // >> RTX6000 nodes:
                // "hydra.launcher.queue=CUDA",
                // "hydra.launcher.template=RTX6000",
                "hydra.launcher.memory=32",
                "hydra.launcher.ncpus=2", // <-- Make sure not to use more than 2 CPUs per GPU on the GTX1080TI and RTX6000 nodes.
                "hydra.launcher.ngpus=1",
                // Notes:
                // - Walltime of 72:00:00 works on our nodes.
                // - In general, on HPC cluster, there is a walltime limit for GPU jobs.
                "hydra.launcher.walltime=59:59:59",
                // "hydra.launcher.walltime=72:00:00", // <-- We choose to run the training for long enough so that we can get up to 500k global steps.
                //    >> END: Hydra options
                //
                // "dataset=mod_multiply_dataset",
                // "dataset=mod_subtract_dataset",
                "dataset=mod_sum_dataset",
                // "dataset.p=97", // <-- 97 is prime
                "dataset.p=197", // <-- 197 is prime
                //
                // "dataset.dataset_seed=42",
                // "dataset.dataset_seed=43,44,45", // <-- Three different seeds
                // "dataset.dataset_seed=46,47",
                "dataset.dataset_seed=48,49,50",
                // "dataset.dataset_seed=48",
                // "dataset.dataset_seed=46,47,48",
                // "dataset.dataset_seed=49",
                //
                // "dataset.frac_train=0.5", // <-- Single choice for testing submission
                // "dataset.frac_train=0.1,0.2,0.3,0.4,0.5", // <-- Different dataset portions
                "dataset.frac_train=0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5", // <-- Different dataset portions (fine-grained)
                //
                "train.optimizer.weight_decay=0.01", // <-- Small weight decay
                // "train.optimizer.eps=1e-8", // <-- eps=1e-8 is the default value in AdamW optimizer
                "train.optimizer.eps=1e-6", // <-- We set eps to a larger value to see if this helps with instabilities in the training process
                "train.optimizer.clip_grad_norm_max_norm=1.0",
                //
                // "train.lr_scheduler.lr_scheduler_type=constant",
                "train.lr_scheduler.lr_scheduler_type=linear",
                //
                // > Parameters for long runs
                "train.max_steps=400000", // <-- 400k steps
                // "train.max_steps=600000", // <-- 600k steps
                // "train.max_steps=800000", // <-- 800k steps (everything interesting should happen before that)
                // "train.max_steps=1500000", // <-- 1.5 million steps
                // "train.max_steps=2000000", // <-- 2 million steps do not finish in time for walltime 72:00:00 and topological_analysis.compute_estimates_every=500
                "train.save_checkpoints_every=50000",
                //
                // "logging.training.create_plot_of_model_output_parameters_every=5000",
                "logging.training.create_plot_of_model_output_parameters_every=10000",
                //
                "topological_analysis.compute_estimates_every=500", // <-- compute local estimates only in large intervals
                "topological_analysis.absolute_n_neighbors_choices=[64]",
                //
                "topological_analysis.create_projection_plot_every=20000", // <-- create projection plot only in large intervals
                //
                "wandb.use_wandb=true",
                // "wandb.wandb_project=grokking_replica_HPC_cluster_runs_different_dataset_portions_long",
                "wandb.wandb_project=grokking_replica_HPC_cluster_runs_different_dataset_portions_long_large_p_new_topological_analysis",
            ],
            "env": {
                "PYTORCH_ENABLE_MPS_FALLBACK": "1"
            }
        },
    ]
}